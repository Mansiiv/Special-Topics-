{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwu0CToyC1ta",
        "outputId": "168b1ded-d9a1-433d-ba9e-f95fb8492ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m2.3/2.5 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q openai langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from langchain_community.llms import OpenAI as LangChainOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from typing import List\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "TXzxqiRbC2bA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_api_key = userdata.get('OPENAI_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_api_key\n",
        ""
      ],
      "metadata": {
        "id": "dYl_9PO1C70u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Clients"
      ],
      "metadata": {
        "id": "qmx9BBSVDD5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=open_ai_api_key)\n",
        ""
      ],
      "metadata": {
        "id": "gTA5dAC5C9JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test prompts\n",
        "def test_prompt(prompt, model=\"gpt-3.5-turbo\", max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0\n",
        "    )\n",
        "    returned_response = response.choices[0].message.content.strip()\n",
        "    return returned_response"
      ],
      "metadata": {
        "id": "YCqGjcVNDSRb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement OpenAI Examples with OpenAI API"
      ],
      "metadata": {
        "id": "VVcYocfODVQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt(prompt, model=\"gpt-3.5-turbo\", max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0  # Set temperature to 0 for deterministic output\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        ""
      ],
      "metadata": {
        "id": "uHjKgguMDFkM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization Example"
      ],
      "metadata": {
        "id": "x26kLCqaDaan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test prompts\n",
        "def test_prompt(prompt, client, model=\"gpt-3.5-turbo\", max_tokens=500):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the OpenAI API and returns the response.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt to send to the API.\n",
        "        client (OpenAI): The OpenAI client object.\n",
        "        model (str, optional): The OpenAI model to use. Defaults to \"gpt-3.5-turbo\".\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the response. Defaults to 500.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the OpenAI API.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0  # Set temperature to 0 for deterministic output\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Summarization Example\n",
        "def summarize_text(text, client):\n",
        "    \"\"\"\n",
        "    Summarizes the given text using the OpenAI API.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to summarize.\n",
        "        client (OpenAI): The OpenAI client object.\n",
        "\n",
        "    Returns:\n",
        "        str: The summarized text.\n",
        "    \"\"\"\n",
        "    return test_prompt(f\"Summarize the following text:\\n\\n{text}\", client)\n",
        "\n",
        "# Test the summarization function\n",
        "long_text = \"\"\"\n",
        "In the rapidly evolving digital era, the complexity\n",
        "and dynamic nature of software applications have significantly\n",
        "increased, driven by ever-changing consumer and business\n",
        "requirements. This shift poses a challenge to traditional software\n",
        "development and deployment methodologies, which often\n",
        "struggle to keep pace with these rapid changes. This paper\n",
        "explores the transition from conventional methods to agile\n",
        "methodologies, emphasizing their critical role in maintaining\n",
        "application stability and facilitating seamless updates with\n",
        "minimal impact on end-users. Central to this study is the\n",
        "examination of automated deployment models, particularly\n",
        "Continuous Integration/Continuous Deployment (CI/CD), and\n",
        "their transformative impact on the software deployment\n",
        "process. The research delves into the intricacies of the DevOps\n",
        "lifecycle, highlighting the importance of various environments\n",
        "such as Development (Dev), Testing (Test), and Production\n",
        "(Prod). These environments are crucial in ensuring that any\n",
        "updated version of an application is rigorously tested and free of\n",
        "bugs before its deployment in a production setting.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize OpenAI client (Make sure you have initialized this elsewhere in your code)\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI(api_key=open_ai_api_key)\n",
        "\n",
        "summary = summarize_text(long_text, client)  # Pass the client object to summarize_text\n",
        "print(\"Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "LasaiD9UDKxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Translation Example\n",
        "\n",
        "def translate_text(text, target_language):\n",
        "    prompt=f\"Translate the following English text to {target_language}:\\n\\n{text}\"\n",
        "    return test_prompt(prompt)\n",
        "\n",
        "# Test the translation function\n",
        "original_text = \"the importance of Dev, Test, and Prod environments in ensuring quality and minimizing disruption during updates.\"\n",
        "translated_text = translate_text(original_text, \"French\")\n",
        "print(\"Original text:\")\n",
        "print(original_text)\n",
        "print(\"\\nTranslated text:\")\n",
        "print(translated_text)"
      ],
      "metadata": {
        "id": "cwwlO-pFDmyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}